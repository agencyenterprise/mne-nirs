
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_12_group_glm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_12_group_glm.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_12_group_glm.py:


.. _tut-fnirs-group:

Group Level GLM
===============

.. sidebar:: Relevant literature

   Gorgolewski, Krzysztof J., et al.
   "The brain imaging data structure, a format for organizing and describing
   outputs of neuroimaging experiments." Scientific data 3.1 (2016): 1-9.

   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).
   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.

This is an example of a group level GLM based fNIRS analysis in MNE-NIRS.

Individual level analysis of this data is described in the
:ref:`MNE fNIRS waveform tutorial <mne:tut-fnirs-processing>`
and the
:ref:`MNE-NIRS fNIRS GLM tutorial <tut-fnirs-hrf>`
So this example will skim over the individual level details
and focus on the group level aspect of analysis.
Here we describe how to process multiple measurements
and summarise  group level effects both as summary statistics and visually.

The data used in this example is available
`at this location <https://github.com/rob-luke/BIDS-NIRS-Tapping>`_.
It is a finger tapping example and is briefly described below.
The dataset contains 5 participants.
The example dataset is in
`BIDS <https://bids.neuroimaging.io>`_
format and therefore already contains
information about triggers, condition names, etc.
The BIDS specification for NIRS data is still under development,
as such you must use the development branch of MNE-BIDS as listed in the
requirements_doc.txt file to run this example.

.. collapse:: |chevron-circle-down| Data description (click to expand)
   :class: success

   Optodes were placed over the motor cortex using the standard NIRX motor
   montage, but with 8 short channels added (see their web page for details).
   To view the sensor locations run
   `raw_intensity.plot_sensors()`.
   A sound was presented to indicate which hand the participant should tap.
   Participants tapped their thumb to their fingers for 5s.
   Conditions were presented in a random order with a randomised inter
   stimulus interval.

.. contents:: Page contents
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 55-93

.. code-block:: default

    # sphinx_gallery_thumbnail_number = 2

    # Authors: Robert Luke <mail@robertluke.net>
    #
    # License: BSD (3-clause)


    # Import common libraries
    import numpy as np
    import pandas as pd

    # Import MNE processing
    from mne.preprocessing.nirs import optical_density, beer_lambert_law

    # Import MNE-NIRS processing
    from mne_nirs.statistics import run_GLM
    from mne_nirs.experimental_design import make_first_level_design_matrix
    from mne_nirs.statistics import glm_region_of_interest, statsmodels_to_results
    from mne_nirs.statistics import compute_contrast
    from mne_nirs.channels import get_short_channels, get_long_channels
    from mne_nirs.channels import picks_pair_to_idx
    from mne_nirs.utils._io import glm_to_tidy
    from mne_nirs.visualisation import plot_glm_group_topo
    from mne_nirs.datasets import fnirs_motor_group

    # Import MNE-BIDS processing
    from mne_bids import BIDSPath, read_raw_bids

    # Import StatsModels
    import statsmodels.formula.api as smf

    # Import Plotting Library
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    from lets_plot import *
    LetsPlot.setup_html()









.. GENERATED FROM PYTHON SOURCE LINES 94-120

Define individual analysis
--------------------------

.. sidebar:: Individual analysis procedures

   Waveform individual analysis:
   :ref:`MNE docs <mne:tut-fnirs-processing>`

   GLM individual analysis:
   :ref:`MNE-NIRS docs <tut-fnirs-hrf>`

First we define the analysis that will be applied to each file.
This is a GLM analysis as described in the
:ref:`individual GLM tutorial <tut-fnirs-hrf>`,
so this example will skim over the individual level details.

The analysis extracts a response estimate for each channel,
each region of interest, and computes a contrast between left and right
finger tapping.
We return the raw object and data frames for the computed results.
Information about channels, triggers and their meanings are stored in the
BIDS structure and are automatically obtained when importing the data.

Here we also resample to a 0.3 Hz sample rate just to speed up the example
and use less memory, resampling to 0.6 Hz is a better choice for full
analyses.

.. GENERATED FROM PYTHON SOURCE LINES 120-181

.. code-block:: default



    def individual_analysis(bids_path, ID):

        raw_intensity = read_raw_bids(bids_path=bids_path, verbose=False)

        # Convert signal to haemoglobin and resample
        raw_od = optical_density(raw_intensity)
        raw_haemo = beer_lambert_law(raw_od)
        raw_haemo.resample(0.3)

        # Cut out just the short channels for creating a GLM repressor
        sht_chans = get_short_channels(raw_haemo)
        raw_haemo = get_long_channels(raw_haemo)

        # Create a design matrix
        design_matrix = make_first_level_design_matrix(raw_haemo, stim_dur=5.0)

        # Append short channels mean to design matrix
        design_matrix["ShortHbO"] = np.mean(sht_chans.copy().pick(picks="hbo").get_data(), axis=0)
        design_matrix["ShortHbR"] = np.mean(sht_chans.copy().pick(picks="hbr").get_data(), axis=0)

        # Run GLM
        glm_est = run_GLM(raw_haemo, design_matrix)

        # Define channels in each region of interest
        # List the channel pairs manually
        left = [[4, 3], [1, 3], [3, 3], [1, 2], [2, 3], [1, 1]]
        right = [[6, 7], [5, 7], [7, 7], [5, 6], [6, 7], [5, 5]]
        # Then generate the correct indices for each pair
        groups = dict(
            Left_Hemisphere=picks_pair_to_idx(raw_haemo, left, on_missing='ignore'),
            Right_Hemisphere=picks_pair_to_idx(raw_haemo, right, on_missing='ignore'))

        # Extract channel metrics
        cha = glm_to_tidy(raw_haemo, glm_est, design_matrix)
        cha["ID"] = ID  # Add the participant ID to the dataframe

        # Compute region of interest results from channel data
        roi = pd.DataFrame()
        for idx, col in enumerate(design_matrix.columns):
            roi = roi.append(glm_region_of_interest(glm_est, groups, idx, col))
        roi["ID"] = ID  # Add the participant ID to the dataframe

        # Contrast left vs right tapping
        contrast_matrix = np.eye(design_matrix.shape[1])
        basic_conts = dict([(column, contrast_matrix[i])
                            for i, column in enumerate(design_matrix.columns)])
        contrast_LvR = basic_conts['Tapping/Left'] - basic_conts['Tapping/Right']
        contrast = compute_contrast(glm_est, contrast_LvR)
        con = glm_to_tidy(raw_haemo, contrast, design_matrix)
        con["ID"] = ID  # Add the participant ID to the dataframe

        # Convert to uM for nicer plotting below.
        cha["theta"] = [t * 1.e6 for t in cha["theta"]]
        roi["theta"] = [t * 1.e6 for t in roi["theta"]]
        con["effect"] = [t * 1.e6 for t in con["effect"]]

        return raw_haemo, roi, cha, con









.. GENERATED FROM PYTHON SOURCE LINES 182-189

Run analysis on all participants
--------------------------------

Next we loop through the five measurements and run the individual analysis
on each. We append the individual results in to a large dataframe that
will contain the results from all measurements. We create a group dataframe
for the region of interest, channel level, and contrast results.

.. GENERATED FROM PYTHON SOURCE LINES 189-211

.. code-block:: default


    df_roi = pd.DataFrame()  # To store region of interest results
    df_cha = pd.DataFrame()  # To store channel level results
    df_con = pd.DataFrame()  # To store channel level contrast results

    for sub in range(1, 6):  # Loop from first to fifth subject
        ID = '%02d' % sub  # Tidy the subject name

        # Create path to file based on experiment info
        bids_path = BIDSPath(subject=ID, task="tapping",
                             root=fnirs_motor_group.data_path(),
                             datatype="nirs", suffix="nirs", extension=".snirf")

        # Analyse data and return both ROI and channel results
        raw_haemo, roi, channel, con = individual_analysis(bids_path, ID)

        # Append individual results to all participants
        df_roi = df_roi.append(roi)
        df_cha = df_cha.append(channel)
        df_con = df_con.append(con)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Using default location ~/mne_data for None...
    Downloading archive MNE-fNIRS-motor-group-data.zip to /home/runner/mne_data
    Downloading https://codeload.github.com/rob-luke/BIDS-NIRS-Tapping/zip/master (41.7 MB)
      0%|          | Downloading : 0.00/41.7M [00:00<?,        ?B/s]      0%|          | Downloading : 24.0k/41.7M [00:00<01:57,     372kB/s]      0%|          | Downloading : 40.0k/41.7M [00:00<02:00,     363kB/s]      0%|          | Downloading : 88.0k/41.7M [00:00<01:57,     372kB/s]      1%|          | Downloading : 312k/41.7M [00:00<01:51,     389kB/s]       1%|1         | Downloading : 440k/41.7M [00:00<01:46,     406kB/s]      2%|1         | Downloading : 824k/41.7M [00:00<01:40,     425kB/s]      6%|6         | Downloading : 2.55M/41.7M [00:00<01:31,     447kB/s]      9%|8         | Downloading : 3.55M/41.7M [00:00<01:25,     470kB/s]     16%|#5        | Downloading : 6.55M/41.7M [00:00<01:14,     495kB/s]     30%|###       | Downloading : 12.6M/41.7M [00:00<00:58,     520kB/s]     44%|####4     | Downloading : 18.6M/41.7M [00:00<00:44,     547kB/s]     49%|####9     | Downloading : 20.6M/41.7M [00:00<00:38,     576kB/s]     54%|#####4    | Downloading : 22.6M/41.7M [00:01<00:33,     606kB/s]     59%|#####8    | Downloading : 24.6M/41.7M [00:01<00:28,     637kB/s]     64%|######3   | Downloading : 26.6M/41.7M [00:01<00:23,     670kB/s]     68%|######8   | Downloading : 28.6M/41.7M [00:01<00:19,     705kB/s]     73%|#######3  | Downloading : 30.6M/41.7M [00:01<00:15,     741kB/s]     78%|#######8  | Downloading : 32.6M/41.7M [00:01<00:12,     780kB/s]     83%|########2 | Downloading : 34.6M/41.7M [00:01<00:09,     820kB/s]     88%|########7 | Downloading : 36.6M/41.7M [00:01<00:06,     862kB/s]     92%|#########2| Downloading : 38.6M/41.7M [00:01<00:03,     907kB/s]     97%|#########7| Downloading : 40.6M/41.7M [00:01<00:01,     953kB/s]    100%|##########| Downloading : 41.7M/41.7M [00:01<00:00,    30.1MB/s]
    Verifying hash 60472f83805b5676730e0d256fabeb7d.
    Decompressing the archive: /home/runner/mne_data/MNE-fNIRS-motor-group-data.zip
    (please be patient, this can take some time)
    Loading /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-01/nirs/sub-01_task-tapping_nirs.snirf
    Reading events from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-01/nirs/sub-01_task-tapping_events.tsv.
    Reading channel info from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-01/nirs/sub-01_task-tapping_channels.tsv.
    Reading 0 ... 23238  =      0.000 ...  2974.464 secs...
    Using default location ~/mne_data for None...
    Loading /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-02/nirs/sub-02_task-tapping_nirs.snirf
    Reading events from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-02/nirs/sub-02_task-tapping_events.tsv.
    Reading channel info from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-02/nirs/sub-02_task-tapping_channels.tsv.
    Reading 0 ... 18877  =      0.000 ...  2416.256 secs...
    Using default location ~/mne_data for None...
    Loading /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-03/nirs/sub-03_task-tapping_nirs.snirf
    Reading events from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-03/nirs/sub-03_task-tapping_events.tsv.
    Reading channel info from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-03/nirs/sub-03_task-tapping_channels.tsv.
    Reading 0 ... 18874  =      0.000 ...  2415.872 secs...
    Using default location ~/mne_data for None...
    Loading /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-04/nirs/sub-04_task-tapping_nirs.snirf
    Reading events from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-04/nirs/sub-04_task-tapping_events.tsv.
    Reading channel info from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-04/nirs/sub-04_task-tapping_channels.tsv.
    Reading 0 ... 23120  =      0.000 ...  2959.360 secs...
    Using default location ~/mne_data for None...
    Loading /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-05/nirs/sub-05_task-tapping_nirs.snirf
    Reading events from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-05/nirs/sub-05_task-tapping_events.tsv.
    Reading channel info from /home/runner/mne_data/MNE-fNIRS-motor-group-data/sub-05/nirs/sub-05_task-tapping_channels.tsv.
    Reading 0 ... 23006  =      0.000 ...  2944.768 secs...




.. GENERATED FROM PYTHON SOURCE LINES 212-221

Visualise Individual results
----------------------------

First we visualise the results from each individual to ensure the
data values look reasonable.
Here we see that we have data from five participants, we plot just the HbO
values and observe they are in the expect range.
We can already see that the control condition is always near zero,
and that the responses look to be contralateral to the tapping hand.

.. GENERATED FROM PYTHON SOURCE LINES 221-232

.. code-block:: default


    grp_results = df_roi.query("Condition in ['Control', 'Tapping/Left', 'Tapping/Right']")
    grp_results = grp_results.query("Chroma in ['hbo']")

    ggplot(grp_results, aes(x='Condition', y='theta', color='ROI', shape='ROI')) \
        + geom_hline(y_intercept=0, linetype="dashed", size=1) \
        + geom_point(size=5) \
        + facet_grid('ID') \
        + ggsize(800, 300)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <html lang="en">
       <head>
           <script type="text/javascript" data-lets-plot-script="library" src="https://dl.bintray.com/jetbrains/lets-plot/lets-plot-1.5.4.min.js"></script>
       </head>
       <body>
              <div id="DNxfoU"></div>
       <script type="text/javascript" data-lets-plot-script="plot">
           var plotSpec={
    'data':{
    'Condition':["Control","Control","Tapping/Left","Tapping/Left","Tapping/Right","Tapping/Right","Control","Control","Tapping/Left","Tapping/Left","Tapping/Right","Tapping/Right","Control","Control","Tapping/Left","Tapping/Left","Tapping/Right","Tapping/Right","Control","Control","Tapping/Left","Tapping/Left","Tapping/Right","Tapping/Right","Control","Control","Tapping/Left","Tapping/Left","Tapping/Right","Tapping/Right"],
    'ROI':["Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere"],
    'theta':[0.37247188799086683,0.25408207236541713,2.242849661285841,5.50136088701287,7.075323444994607,2.9545358935528223,-0.8996860279166524,-1.5440764380466974,3.130977426133792,4.205153418778897,7.777819085844279,1.2695468514835004,-0.8860389918563338,-1.9724115271252622,0.9799166999623115,5.653140455892356,3.4549740876208803,3.693797736937878,-0.02847266328575329,-0.1745786970923922,-3.2043576500313686,9.426673639552247,4.997328058024655,-1.4423797732283803,0.03537906168788708,-0.47964545400762376,6.65215970019276,23.359206828682286,23.014624888323674,11.00946180545952],
    'ID':["01","01","01","01","01","01","02","02","02","02","02","02","03","03","03","03","03","03","04","04","04","04","04","04","05","05","05","05","05","05"]
    },
    'mapping':{
    'x':"Condition",
    'y':"theta",
    'color':"ROI",
    'shape':"ROI"
    },
    'data_meta':{
    },
    'facet':{
    'name':"grid",
    'x':"ID"
    },
    'ggsize':{
    'width':800,
    'height':300
    },
    'kind':"plot",
    'scales':[],
    'layers':[{
    'geom':"hline",
    'mapping':{
    },
    'data_meta':{
    },
    'y_intercept':0,
    'linetype':"dashed",
    'size':1,
    'data':{
    }
    },{
    'geom':"point",
    'mapping':{
    },
    'data_meta':{
    },
    'size':5,
    'data':{
    }
    }]
    };
           var plotContainer = document.getElementById("DNxfoU");
           LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);
       </script>
       </body>
    </html>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 233-264

Compute group level results
---------------------------

.. sidebar:: Relevant literature

   For an introduction to mixed effects analysis see:
   Winter, Bodo. "A very basic tutorial for performing linear mixed effects
   analyses." arXiv preprint arXiv:1308.5499 (2013).

   For a summary of linear mixed models in python
   and the relation to lmer see:
   :ref:`statsmodels docs <statsmodels:mixedlmmod>`

   For a summary of these models in the context of fNIRS see section 3.5 of:
   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).
   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.

Next we use a linear mixed effects model to examine the
relation between conditions and our response estimate (theta).
Combinations of 3 fixed effects will be evaluated, ROI (left vs right),
condition (control, tapping/left, tapping/right), and chromophore (HbO, HbR).
With a random effect of subject.
Alternatively, you could export the group dataframe (`df_roi.to_csv()`) and
analyse in your favorite stats program.

We do not explore the modeling procedure in depth here as topics
such model selection and examining residuals are beyond the scope of
this example (see relevant literature).
Alternatively, we could use a robust linear
model by using the code
`roi_model = rlm('theta ~ -1 + ROI:Condition:Chroma', grp_results).fit()`.

.. GENERATED FROM PYTHON SOURCE LINES 264-272

.. code-block:: default


    grp_results = df_roi.query("Condition in ['Control','Tapping/Left', 'Tapping/Right']")

    roi_model = smf.mixedlm("theta ~ -1 + ROI:Condition:Chroma",
                            grp_results, groups=grp_results["ID"]).fit(method='nm')
    roi_model.summary()






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.6/x64/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.
      warnings.warn(msg, ConvergenceWarning)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <table class="simpletable">
    <tr>
           <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>theta</td>  
    </tr>
    <tr>
      <td>No. Observations:</td>   <td>60</td>          <td>Method:</td>         <td>REML</td>   
    </tr>
    <tr>
         <td>No. Groups:</td>       <td>5</td>          <td>Scale:</td>         <td>16.2281</td> 
    </tr>
    <tr>
      <td>Min. group size:</td>    <td>12</td>      <td>Log-Likelihood:</td>   <td>-144.6475</td>
    </tr>
    <tr>
      <td>Max. group size:</td>    <td>12</td>        <td>Converged:</td>         <td>Yes</td>   
    </tr>
    <tr>
      <td>Mean group size:</td>   <td>12.0</td>            <td></td>               <td></td>     
    </tr>
    </table>
    <table class="simpletable">
    <tr>
                                   <td></td>                               <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Control]:Chroma[hbo]</th>        <td>-0.281</td>   <td>1.802</td>  <td>-0.156</td> <td>0.876</td> <td>-3.812</td>  <td>3.250</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Control]:Chroma[hbo]</th>       <td>-0.783</td>   <td>1.802</td>  <td>-0.435</td> <td>0.664</td> <td>-4.314</td>  <td>2.748</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping/Left]:Chroma[hbo]</th>    <td>1.960</td>   <td>1.802</td>   <td>1.088</td> <td>0.277</td> <td>-1.571</td>  <td>5.491</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping/Left]:Chroma[hbo]</th>   <td>9.629</td>   <td>1.802</td>   <td>5.345</td> <td>0.000</td>  <td>6.098</td> <td>13.160</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping/Right]:Chroma[hbo]</th>   <td>9.264</td>   <td>1.802</td>   <td>5.142</td> <td>0.000</td>  <td>5.733</td> <td>12.795</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping/Right]:Chroma[hbo]</th>  <td>3.497</td>   <td>1.802</td>   <td>1.941</td> <td>0.052</td> <td>-0.034</td>  <td>7.028</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Control]:Chroma[hbr]</th>         <td>0.268</td>   <td>1.802</td>   <td>0.149</td> <td>0.882</td> <td>-3.263</td>  <td>3.799</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Control]:Chroma[hbr]</th>        <td>0.103</td>   <td>1.802</td>   <td>0.057</td> <td>0.954</td> <td>-3.428</td>  <td>3.634</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping/Left]:Chroma[hbr]</th>   <td>-2.003</td>   <td>1.802</td>  <td>-1.112</td> <td>0.266</td> <td>-5.534</td>  <td>1.528</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping/Left]:Chroma[hbr]</th>  <td>-4.258</td>   <td>1.802</td>  <td>-2.364</td> <td>0.018</td> <td>-7.789</td> <td>-0.727</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping/Right]:Chroma[hbr]</th>  <td>-4.128</td>   <td>1.802</td>  <td>-2.291</td> <td>0.022</td> <td>-7.658</td> <td>-0.597</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping/Right]:Chroma[hbr]</th> <td>-1.768</td>   <td>1.802</td>  <td>-0.981</td> <td>0.326</td> <td>-5.299</td>  <td>1.763</td>
    </tr>
    <tr>
      <th>Group Var</th>                                                   <td>0.000</td>   <td>0.271</td>     <td></td>      <td></td>       <td></td>       <td></td>   
    </tr>
    </table>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 273-284

Visualise group results
-----------------------

Now we can summarise the output of the second level model.
This figure shows that the control condition has small responses that
are not significantly different to zero for both HbO
and HbR in both hemispheres.
Whereas clear significant responses are show for the two tapping conditions.
We also observe the the tapping response is
larger in the contralateral hemisphere.
Filled symbols represent HbO, unfilled symbols represent HbR.

.. GENERATED FROM PYTHON SOURCE LINES 284-299

.. code-block:: default


    df = statsmodels_to_results(roi_model)

    ggplot(df.query("Chroma == 'hbo'"),
           aes(x='Condition', y='Coef.', color='Significant', shape='ROI')) \
        + geom_hline(y_intercept=0, linetype="dashed", size=1) \
        + geom_point(size=5) \
        + scale_shape_manual(values=[16, 17]) \
        + ggsize(800, 300) \
        + geom_point(data=df.query("Chroma == 'hbr'")
                     .query("ROI == 'Left_Hemisphere'"), size=5, shape=1) \
        + geom_point(data=df.query("Chroma == 'hbr'")
                     .query("ROI == 'Right_Hemisphere'"), size=5, shape=2)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <html lang="en">
       <head>
           <script type="text/javascript" data-lets-plot-script="library" src="https://dl.bintray.com/jetbrains/lets-plot/lets-plot-1.5.4.min.js"></script>
       </head>
       <body>
              <div id="p9oLFh"></div>
       <script type="text/javascript" data-lets-plot-script="plot">
           var plotSpec={
    'data':{
    'Coef.':[-0.281,-0.783,1.96,9.629,9.264,3.497],
    'ROI':["Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere"],
    'Condition':["Control","Control","Tapping/Left","Tapping/Left","Tapping/Right","Tapping/Right"],
    'Significant':[false,false,false,true,true,false]
    },
    'mapping':{
    'x':"Condition",
    'y':"Coef.",
    'color':"Significant",
    'shape':"ROI"
    },
    'data_meta':{
    },
    'ggsize':{
    'width':800,
    'height':300
    },
    'kind':"plot",
    'scales':[{
    'aesthetic':"shape",
    'values':[16,17]
    }],
    'layers':[{
    'geom':"hline",
    'mapping':{
    },
    'data_meta':{
    },
    'y_intercept':0,
    'linetype':"dashed",
    'size':1,
    'data':{
    }
    },{
    'geom':"point",
    'mapping':{
    },
    'data_meta':{
    },
    'size':5,
    'data':{
    }
    },{
    'geom':"point",
    'data':{
    'Coef.':[0.268,-2.003,-4.128],
    'Condition':["Control","Tapping/Left","Tapping/Right"],
    'Significant':[false,false,true]
    },
    'mapping':{
    },
    'data_meta':{
    },
    'size':5,
    'shape':1
    },{
    'geom':"point",
    'data':{
    'Coef.':[0.103,-4.258,-1.768],
    'Condition':["Control","Tapping/Left","Tapping/Right"],
    'Significant':[false,true,false]
    },
    'mapping':{
    },
    'data_meta':{
    },
    'size':5,
    'shape':2
    }]
    };
           var plotContainer = document.getElementById("p9oLFh");
           LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);
       </script>
       </body>
    </html>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 300-309

Group topographic visualisation
-------------------------------

We can also view the topographic representation of the data
(rather than the ROI summary above).
Here we just plot the oxyhaemoglobin for the two tapping conditions.
First we compute the mixed effects model for each channel (rather
than region of interest as above).
Then we pass these results to the topomap function.

.. GENERATED FROM PYTHON SOURCE LINES 309-353

.. code-block:: default


    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10),
                             gridspec_kw=dict(width_ratios=[1, 1]))

    # Cut down the dataframe just to the conditions we are interested in
    ch_summary = df_cha.query("Condition in ['Tapping/Left', 'Tapping/Right']")
    ch_summary = ch_summary.query("Chroma in ['hbo']")

    # Run group level model and convert to dataframe
    ch_model = smf.mixedlm("theta ~ -1 + ch_name:Chroma:Condition",
                           ch_summary, groups=ch_summary["ID"]).fit(method='nm')
    ch_model_df = statsmodels_to_results(ch_model)

    # Plot the two conditions
    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo"),
                        ch_model_df.query("Condition in ['Tapping/Left']"),
                        colorbar=False, axes=axes[0, 0],
                        vmin=0, vmax=20, cmap=mpl.cm.Oranges)

    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo"),
                        ch_model_df.query("Condition in ['Tapping/Right']"),
                        colorbar=True, axes=axes[0, 1],
                        vmin=0, vmax=20, cmap=mpl.cm.Oranges)

    # Cut down the dataframe just to the conditions we are interested in
    ch_summary = df_cha.query("Condition in ['Tapping/Left', 'Tapping/Right']")
    ch_summary = ch_summary.query("Chroma in ['hbr']")

    # Run group level model and convert to dataframe
    ch_model = smf.mixedlm("theta ~ -1 + ch_name:Chroma:Condition",
                           ch_summary, groups=ch_summary["ID"]).fit(method='nm')
    ch_model_df = statsmodels_to_results(ch_model)

    # Plot the two conditions
    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbr"),
                        ch_model_df.query("Condition in ['Tapping/Left']"),
                        colorbar=False, axes=axes[1, 0],
                        vmin=-10, vmax=0, cmap=mpl.cm.Blues_r)
    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbr"),
                        ch_model_df.query("Condition in ['Tapping/Right']"),
                        colorbar=True, axes=axes[1, 1],
                        vmin=-10, vmax=0, cmap=mpl.cm.Blues_r)





.. image:: /auto_examples/images/sphx_glr_plot_12_group_glm_001.png
    :alt: Tapping/Left, Tapping/Right, Tapping/Left, Tapping/Right
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'Tapping/Right'}>



.. GENERATED FROM PYTHON SOURCE LINES 354-360

Contrasts
---------

Finally we can examine the difference between the left and right hand
tapping conditions by viewing the contrast results
in a topographic representation.

.. GENERATED FROM PYTHON SOURCE LINES 360-373

.. code-block:: default


    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))
    con_summary = df_con.query("Chroma in ['hbo']")

    # Run group level model and convert to dataframe
    con_model = smf.mixedlm("effect ~ -1 + ch_name:Chroma",
                            con_summary, groups=con_summary["ID"]).fit(method='nm')
    con_model_df = statsmodels_to_results(con_model)

    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo"),
                        con_model_df, colorbar=True, axes=axes)





.. image:: /auto_examples/images/sphx_glr_plot_12_group_glm_002.png
    :alt: Contrast
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.6/x64/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.
      warnings.warn(msg, ConvergenceWarning)
    /opt/hostedtoolcache/Python/3.8.6/x64/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.
      warnings.warn(msg, ConvergenceWarning)

    <AxesSubplot:title={'center':'Contrast'}>



.. GENERATED FROM PYTHON SOURCE LINES 374-377

Or we can view only the left hemisphere for the contrast.
And set all channels that dont have a significant response to zero.


.. GENERATED FROM PYTHON SOURCE LINES 378-380

.. code-block:: default


    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo").pick(picks=range(10)),
                        con_model_df, colorbar=True, threshold=True)


.. image:: /auto_examples/images/sphx_glr_plot_12_group_glm_003.png
    :alt: Contrast
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Reducing GLM results to match MNE data

    <AxesSubplot:title={'center':'Contrast'}>




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  33.781 seconds)

**Estimated memory usage:**  265 MB


.. _sphx_glr_download_auto_examples_plot_12_group_glm.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_12_group_glm.py <plot_12_group_glm.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_12_group_glm.ipynb <plot_12_group_glm.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
