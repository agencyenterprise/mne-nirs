
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/general/plot_16_waveform_group.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_general_plot_16_waveform_group.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_general_plot_16_waveform_group.py:


.. _tut-fnirs-group-wave:

Group Level Waveform Analysis
=============================

This is an example of a group level waveform based
functional near-infrared spectroscopy (fNIRS)
analysis in MNE-NIRS.

.. sidebar:: Relevant literature

   Luke, Robert, et al.
   "Analysis methods for measuring passive auditory fNIRS responses generated
   by a block-design paradigm." Neurophotonics 8.2 (2021):
   `025008 <https://www.spiedigitallibrary.org/journals/neurophotonics/volume-8/issue-2/025008/Analysis-methods-for-measuring-passive-auditory-fNIRS-responses-generated-by/10.1117/1.NPh.8.2.025008.short>`_.

   Gorgolewski, Krzysztof J., et al.
   "The brain imaging data structure, a format for organizing and describing
   outputs of neuroimaging experiments." Scientific data 3.1 (2016): 1-9.

   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).
   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.

Individual level analysis of this data is described in the
:ref:`MNE-NIRS fNIRS waveform tutorial <tut-fnirs-processing>`
and the
:ref:`MNE-NIRS fNIRS GLM tutorial <tut-fnirs-hrf>`.
As such, this example will skim over the individual level details
and focus on the group level aspects of analysis.
Here we describe how to process multiple measurements
and summarise group level effects both as summary statistics and visually.

The data used in this example is available
`at this location <https://github.com/rob-luke/BIDS-NIRS-Tapping>`_.
It is a finger tapping example and is briefly described below.
The dataset contains 5 participants.
The example dataset is in
`BIDS <https://bids.neuroimaging.io>`_
format and therefore already contains
information about triggers, condition names, etc.
The BIDS specification for NIRS data is still under development,
as such you must use the development branch of MNE-BIDS as listed in the
requirements_doc.txt file to run this example.

.. collapse:: |chevron-circle-down| Data description (click to expand)
   :class: success

   Optodes were placed over the motor cortex using the standard NIRX motor
   montage, but with 8 short channels added (see their web page for details).
   To view the sensor locations run
   `raw_intensity.plot_sensors()`.
   A sound was presented to indicate which hand the participant should tap.
   Participants tapped their thumb to their fingers for 5s.
   Conditions were presented in a random order with a randomised inter
   stimulus interval.

.. contents:: Page contents
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 62-103

.. code-block:: default

    # sphinx_gallery_thumbnail_number = 2

    # Authors: Robert Luke <mail@robertluke.net>
    #
    # License: BSD (3-clause)

    # Import common libraries
    import pandas as pd
    from itertools import compress
    from collections import defaultdict
    from copy import deepcopy
    from pprint import pprint

    # Import MNE processing
    from mne.viz import plot_compare_evokeds
    from mne import Epochs, events_from_annotations, set_log_level

    # Import MNE-NIRS processing
    from mne_nirs.channels import get_long_channels
    from mne_nirs.channels import picks_pair_to_idx
    from mne_nirs.datasets import fnirs_motor_group
    from mne.preprocessing.nirs import beer_lambert_law, optical_density,\
        temporal_derivative_distribution_repair, scalp_coupling_index
    from mne_nirs.signal_enhancement import (enhance_negative_correlation,
                                             short_channel_regression)

    # Import MNE-BIDS processing
    from mne_bids import BIDSPath, read_raw_bids

    # Import StatsModels
    import statsmodels.formula.api as smf

    # Import Plotting Library
    import matplotlib.pyplot as plt
    from lets_plot import *

    # Set general parameters
    set_log_level("WARNING")  # Don't show info, as is repetitive for many subjects
    LetsPlot.setup_html()









.. GENERATED FROM PYTHON SOURCE LINES 104-118

Define individual analysis
--------------------------

.. sidebar:: Individual analysis procedures

   :ref:`Waveform individual analysis <tut-fnirs-processing>`

   :ref:`GLM individual analysis <tut-fnirs-hrf>`

First we define the analysis that will be applied to each file.
This is a waveform analysis as described in the
:ref:`individual waveform tutorial <tut-fnirs-processing>`
and :ref:`artifact correction tutorial <ex-fnirs-artifacts>`.
As such, this example will skim over the individual level details.

.. GENERATED FROM PYTHON SOURCE LINES 118-157

.. code-block:: default


    def individual_analysis(bids_path):

        # Read data with annotations in BIDS format
        raw_intensity = read_raw_bids(bids_path=bids_path, verbose=False)

        # Convert signal to optical density and determine bad channels
        raw_od = optical_density(raw_intensity)
        sci = scalp_coupling_index(raw_od, h_freq=1.35, h_trans_bandwidth=0.1)
        raw_od.info["bads"] = list(compress(raw_od.ch_names, sci < 0.5))
        raw_od.interpolate_bads()

        # Downsample and apply signal cleaning techniques
        raw_od.resample(0.8)
        raw_od = temporal_derivative_distribution_repair(raw_od)
        raw_od = short_channel_regression(raw_od)

        # Convert to haemoglobin and filter
        raw_haemo = beer_lambert_law(raw_od)
        raw_haemo = raw_haemo.filter(0.02, 0.3,
                                     h_trans_bandwidth=0.1, l_trans_bandwidth=0.01,
                                     verbose=False)

        # Apply further data cleaning techniques and extract epochs
        raw_haemo = enhance_negative_correlation(raw_haemo)
        raw_haemo = get_long_channels(raw_haemo, min_dist=0.01, max_dist=0.05)

        # Extract events but ignore those with
        # the word ends (i.e. drop ExperimentEnds events)
        events, event_dict = events_from_annotations(raw_haemo, verbose=False,
                                                     regexp='^(?![Ends]).*$')
        epochs = Epochs(raw_haemo, events, event_id=event_dict, tmin=-5, tmax=20,
                        reject=dict(hbo=200e-6), reject_by_annotation=True,
                        proj=True, baseline=(None, 0), detrend=0,
                        preload=True, verbose=False)

        return raw_haemo, epochs









.. GENERATED FROM PYTHON SOURCE LINES 158-166

Run analysis on all data
------------------------

Next we loop through the five measurements and run the individual analysis
on each. For each individual the function returns the raw data and an
epoch structure. The epoch structure is then averaged to obtain an evoked
response per participant. The individual evoked data is stored in a
dictionary (`all_evokeds`) by condition.

.. GENERATED FROM PYTHON SOURCE LINES 166-184

.. code-block:: default


    all_evokeds = defaultdict(list)

    for sub in range(1, 6):  # Loop from first to fifth subject

        # Create path to file based on experiment info
        bids_path = BIDSPath(subject="%02d" % sub, task="tapping", datatype="nirs",
                             root=fnirs_motor_group.data_path(), suffix="nirs",
                             extension=".snirf")

        # Analyse data and return both ROI and channel results
        raw_haemo, epochs = individual_analysis(bids_path)

        # Save evoked individual participant data along with others in all_evokeds
        for cidx, condition in enumerate(epochs.event_id):
            all_evokeds[condition].append(epochs[condition].average())






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /tmp/tmppait051g/4dc0217c03d606ee85e871140140c0162ec4c19e/examples/general/plot_16_waveform_group.py:128: RuntimeWarning: No bad channels to interpolate. Doing nothing...
      raw_od.interpolate_bads()




.. GENERATED FROM PYTHON SOURCE LINES 185-190

The end result is a dictionary indexed per condition.
With each item in the dictionary being a list of evoked responses.
See below that for each condition we have obtained an MNE evoked type
that is generated from the average of 30 trials and epoched from -5 to
20 seconds.

.. GENERATED FROM PYTHON SOURCE LINES 190-193

.. code-block:: default


    pprint(all_evokeds)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    defaultdict(<class 'list'>,
                {'Control': [<Evoked | 'Control' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                             <Evoked | 'Control' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                             <Evoked | 'Control' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                             <Evoked | 'Control' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                             <Evoked | 'Control' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>],
                 'Tapping/Left': [<Evoked | 'Tapping/Left' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                  <Evoked | 'Tapping/Left' (average, N=29), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                  <Evoked | 'Tapping/Left' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                  <Evoked | 'Tapping/Left' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                  <Evoked | 'Tapping/Left' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>],
                 'Tapping/Right': [<Evoked | 'Tapping/Right' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                   <Evoked | 'Tapping/Right' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                   <Evoked | 'Tapping/Right' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                   <Evoked | 'Tapping/Right' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>,
                                   <Evoked | 'Tapping/Right' (average, N=30), -5 – 20 sec, baseline -5 – 0 sec, 40 ch, ~62 kB>]})




.. GENERATED FROM PYTHON SOURCE LINES 194-200

View average waveform
---------------------

Next a grand average epoch waveform is generated per condition.
This is generated using all long fNIRS channels, as illustrated in the head
inset.

.. GENERATED FROM PYTHON SOURCE LINES 200-214

.. code-block:: default


    # Specify the figure size and limits per chromophore.
    fig, axes = plt.subplots(nrows=1, ncols=len(all_evokeds), figsize=(17, 5))
    lims = dict(hbo=[-5, 12], hbr=[-5, 12])

    for (pick, color) in zip(['hbo', 'hbr'], ['r', 'b']):
        for idx, evoked in enumerate(all_evokeds):
            plot_compare_evokeds({evoked: all_evokeds[evoked]}, combine='mean',
                                 picks=pick, axes=axes[idx], show=False,
                                 colors=[color], legend=False, ylim=lims, ci=0.95,
                                 show_sensors=idx == 2)
            axes[idx].set_title('{}'.format(evoked))
    axes[0].legend(["Oxyhaemoglobin", "Deoxyhaemoglobin"])




.. image-sg:: /auto_examples/general/images/sphx_glr_plot_16_waveform_group_001.png
   :alt: Control, Tapping/Left, Tapping/Right
   :srcset: /auto_examples/general/images/sphx_glr_plot_16_waveform_group_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f1729dbe6a0>



.. GENERATED FROM PYTHON SOURCE LINES 215-219

From this figure we observe that the response to the tapping condition
with the right hand seems larger than when no tapping occurred in the control
condition (similar for tapping with the left hand).
We test if this is the case in the analysis below.

.. GENERATED FROM PYTHON SOURCE LINES 222-245

Generate regions of interest
--------------------------------
.. sidebar:: Relevant literature

   Zimeo Morais, G.A., Balardin, J.B. & Sato, J.R.
   fNIRS Optodes’ Location Decider (fOLD): a toolbox for probe arrangement
   guided by brain regions-of-interest. Sci Rep 8, 3341 (2018).

   Shader and Luke et al. "The use of broad vs restricted regions of
   interest in functional near-infrared spectroscopy for measuring cortical
   activation to auditory-only and visual-only speech."
   Hearing Research (2021): `108256 <https://www.sciencedirect.com/science/article/pii/S0378595521000903>`_.

Here we specify two regions of interest by listing out the source-detector
pairs of interest and then determining which channels these correspond to
within the raw data structure. The channel indices are stored in a
dictionary for access below.
The fOLD toolbox can be used to assist in the design of ROIs.
And consideration should be paid to ensure optimal size ROIs are selected.

In this example two ROIs are generated. One for the left motor cortex,
and one for the right motor cortex. These are called `Left_Hemisphere` and
`Right_Hemisphere` and stored in the `rois` dictionary.

.. GENERATED FROM PYTHON SOURCE LINES 245-257

.. code-block:: default


    # Specify channel pairs for each ROI
    left = [[4, 3], [1, 3], [3, 3], [1, 2], [2, 3], [1, 1]]
    right = [[8, 7], [5, 7], [7, 7], [5, 6], [6, 7], [5, 5]]

    # Then generate the correct indices for each pair and store in dictionary
    rois = dict(
        Left_Hemisphere=picks_pair_to_idx(raw_haemo, left, on_missing='ignore'),
        Right_Hemisphere=picks_pair_to_idx(raw_haemo, right, on_missing='ignore'))

    pprint(rois)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    {'Left_Hemisphere': [16, 17, 4, 5, 14, 15, 2, 3, 8, 9, 0, 1],
     'Right_Hemisphere': [36, 37, 24, 25, 34, 35, 22, 23, 28, 29, 20, 21]}




.. GENERATED FROM PYTHON SOURCE LINES 258-264

Create average waveform per ROI
-------------------------------

Next an average waveform is generated per condition per region of interest.
This allows the researcher to view the responses elicited in different
regions of the brain per condition.

.. GENERATED FROM PYTHON SOURCE LINES 264-287

.. code-block:: default


    # Specify the figure size and limits per chromophore.
    fig, axes = plt.subplots(nrows=len(rois), ncols=len(all_evokeds),
                             figsize=(15, 6))
    lims = dict(hbo=[-8, 16], hbr=[-8, 16])

    for (pick, color) in zip(['hbo', 'hbr'], ['r', 'b']):
        for ridx, roi in enumerate(rois):
            for cidx, evoked in enumerate(all_evokeds):
                if pick == 'hbr':
                    picks = rois[roi][1::2]  # Select only the hbr channels
                else:
                    picks = rois[roi][0::2]  # Select only the hbo channels

                plot_compare_evokeds({evoked: all_evokeds[evoked]}, combine='mean',
                                     picks=picks, axes=axes[ridx, cidx],
                                     show=False, colors=[color], legend=False,
                                     ylim=lims, ci=0.95, show_sensors=cidx == 2)
                axes[0, cidx].set_title(f"{evoked}")
                axes[1, cidx].set_title("") 
            axes[ridx, 0].set_ylabel(f"{roi}\nChromophore (ΔμMol)")
    axes[0, 0].legend(["Oxyhaemoglobin", "Deoxyhaemoglobin"])




.. image-sg:: /auto_examples/general/images/sphx_glr_plot_16_waveform_group_002.png
   :alt: Control, Tapping/Left, Tapping/Right
   :srcset: /auto_examples/general/images/sphx_glr_plot_16_waveform_group_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f172b92bb80>



.. GENERATED FROM PYTHON SOURCE LINES 288-291

From this figure we observe that the response to the tapping seems
largest in the brain region contralateral to the tapping.
We test if this is the case in the analysis below.

.. GENERATED FROM PYTHON SOURCE LINES 294-304

Extract evoked amplitude
------------------------

The waveforms above provide a qualitative overview of the data.
It is also useful to perform a quantitative analysis based on features in
the dataset. Here we extract the average value of the waveform between
5 and 7 seconds for each subject, condition, region of interest, and
chromophore. This data is stored in a dataframe. The dataframe is saved
to a csv for easy analysis in any statistical analysis software.
We also demonstrate two example analysis on these values below.

.. GENERATED FROM PYTHON SOURCE LINES 304-327

.. code-block:: default


    df = pd.DataFrame(columns=['ID', 'ROI', 'Chroma', 'Condition', 'Value'])

    for idx, evoked in enumerate(all_evokeds):
        for subj_data in all_evokeds[evoked]:
            for roi in rois:
                for chroma in ["hbo", "hbr"]:
                    subj_id = subj_data.info["subject_info"]['first_name']
                    data = deepcopy(subj_data).pick(picks=rois[roi]).pick(chroma)
                    value = data.crop(tmin=5.0, tmax=7.0).data.mean() * 1.0e6

                    # Append metadata and extracted feature to dataframe
                    df = df.append({'ID': subj_id, 'ROI': roi, 'Chroma': chroma,
                                    'Condition': evoked, 'Value': value},
                                   ignore_index=True)

    # You can export the dataframe for analysis in your favorite stats program
    df.to_csv("stats-export.csv")

    # Print out the first entries in the dataframe
    df.head()







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>ID</th>
          <th>ROI</th>
          <th>Chroma</th>
          <th>Condition</th>
          <th>Value</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>P1</td>
          <td>Left_Hemisphere</td>
          <td>hbo</td>
          <td>Control</td>
          <td>-0.361710</td>
        </tr>
        <tr>
          <th>1</th>
          <td>P1</td>
          <td>Left_Hemisphere</td>
          <td>hbr</td>
          <td>Control</td>
          <td>0.097388</td>
        </tr>
        <tr>
          <th>2</th>
          <td>P1</td>
          <td>Right_Hemisphere</td>
          <td>hbo</td>
          <td>Control</td>
          <td>-0.174879</td>
        </tr>
        <tr>
          <th>3</th>
          <td>P1</td>
          <td>Right_Hemisphere</td>
          <td>hbr</td>
          <td>Control</td>
          <td>0.049573</td>
        </tr>
        <tr>
          <th>4</th>
          <td>P2</td>
          <td>Left_Hemisphere</td>
          <td>hbo</td>
          <td>Control</td>
          <td>0.006949</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 328-340

View individual results
-----------------------

This figure simply summarises the information in the dataframe created above.
We observe that the values extracted from the waveform for the control
condition generally sit around 0. Whereas the tapping conditions have
larger values. There is quite some spread in the values for the tapping
conditions, this is typical of a group study. Many factors affect the
response amplitude in an fNIRS experiment including skin thickness,
skull thickness, both of which vary across the head and across participants.
For this reason fNIRS is most appropriate for detecting changes within a
single ROI between conditions.

.. GENERATED FROM PYTHON SOURCE LINES 340-349

.. code-block:: default


    ggplot(df.query("Chroma == 'hbo'"),
           aes(x='Condition', y='Value', color='ID', shape='ROI')) \
        + geom_hline(y_intercept=0, linetype="dashed", size=1) \
        + geom_point(size=5) \
        + scale_shape_manual(values=[16, 17]) \
        + ggsize(800, 300)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <html lang="en">
       <head>
           <script type="text/javascript" data-lets-plot-script="library" src="https://cdnjs.cloudflare.com/ajax/libs/lets-plot/2.0.2/lets-plot.min.js"></script>
       </head>
       <body>
              <div id="EmmAjh"></div>
       <script type="text/javascript" data-lets-plot-script="plot">
           var plotSpec={
    'data':{
    'ID':["P1","P1","P2","P2","P3","P3","P4","P4","P5","P5","P1","P1","P2","P2","P3","P3","P4","P4","P5","P5","P1","P1","P2","P2","P3","P3","P4","P4","P5","P5"],
    'ROI':["Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere"],
    'Condition':["Control","Control","Control","Control","Control","Control","Control","Control","Control","Control","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Left","Tapping/Right","Tapping/Right","Tapping/Right","Tapping/Right","Tapping/Right","Tapping/Right","Tapping/Right","Tapping/Right","Tapping/Right","Tapping/Right"],
    'Value':[-0.36171035106247945,-0.1748785859074037,0.00694859845939739,-0.12254052088058381,-0.008625826466316445,0.6201879757179695,0.38453914833237257,0.23462047002809786,0.8232055465206102,1.3671532109069244,8.109055235504849,13.517132115552439,3.717871713157761,4.973709586448003,2.4579163431979714,2.8164672848884362,5.439843344516769,16.478856720123055,4.895892596155512,12.748167542176516,10.114996699691915,7.882978135065416,5.74378381537993,2.4401233363787735,1.6446053940585814,0.9741334959796175,5.5068544378116115,4.222640426408192,10.030036509950502,9.03682576253993]
    },
    'mapping':{
    'x':"Condition",
    'y':"Value",
    'color':"ID",
    'shape':"ROI"
    },
    'data_meta':{
    },
    'ggsize':{
    'width':800,
    'height':300
    },
    'kind':"plot",
    'scales':[{
    'aesthetic':"shape",
    'values':[16,17]
    }],
    'layers':[{
    'geom':"hline",
    'mapping':{
    },
    'data_meta':{
    },
    'y_intercept':0,
    'linetype':"dashed",
    'size':1,
    'data':{
    }
    },{
    'geom':"point",
    'mapping':{
    },
    'data_meta':{
    },
    'size':5,
    'data':{
    }
    }]
    };
           var plotContainer = document.getElementById("EmmAjh");
           LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);
       </script>
       </body>
    </html>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 350-357

Research question 1: Comparison of conditions
---------------------------------------------------------------------------------------------------

In this example question we ask: is the hbo responsen the left ROI to tapping
with the right hand larger than the response when not tapping (control)?
For this token example we subset the dataframe and then apply the mixed
effect model.

.. GENERATED FROM PYTHON SOURCE LINES 357-366

.. code-block:: default


    input_data = df.query("Condition in ['Control', 'Tapping/Right']")
    input_data = input_data.query("Chroma in ['hbo']")
    input_data = input_data.query("ROI in ['Left_Hemisphere']")

    roi_model = smf.mixedlm("Value ~ Condition", input_data,
                            groups=input_data["ID"]).fit()
    roi_model.summary()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <table class="simpletable">
    <tr>
           <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>Value</td> 
    </tr>
    <tr>
      <td>No. Observations:</td>   <td>10</td>          <td>Method:</td>         <td>REML</td>  
    </tr>
    <tr>
         <td>No. Groups:</td>       <td>5</td>          <td>Scale:</td>         <td>6.1366</td> 
    </tr>
    <tr>
      <td>Min. group size:</td>     <td>2</td>      <td>Log-Likelihood:</td>   <td>-20.3990</td>
    </tr>
    <tr>
      <td>Max. group size:</td>     <td>2</td>        <td>Converged:</td>         <td>Yes</td>  
    </tr>
    <tr>
      <td>Mean group size:</td>    <td>2.0</td>            <td></td>               <td></td>    
    </tr>
    </table>
    <table class="simpletable">
    <tr>
                   <td></td>              <th>Coef.</th> <th>Std.Err.</th>   <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>
    </tr>
    <tr>
      <th>Intercept</th>                  <td>0.169</td>   <td>1.134</td>  <td>0.149</td> <td>0.882</td> <td>-2.053</td>  <td>2.391</td>
    </tr>
    <tr>
      <th>Condition[T.Tapping/Right]</th> <td>6.439</td>   <td>1.567</td>  <td>4.110</td> <td>0.000</td>  <td>3.368</td>  <td>9.510</td>
    </tr>
    <tr>
      <th>Group Var</th>                  <td>0.291</td>   <td>1.356</td>    <td></td>      <td></td>       <td></td>       <td></td>   
    </tr>
    </table>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 367-370

The model indicates that for the oxyhaemoglobin data in the left
region of interest, that the tapping condition with the right hand evokes
a 6.4 μM larger response than the control.

.. GENERATED FROM PYTHON SOURCE LINES 373-383

Research question 2: Are responses larger on the contralateral side to tapping?
-------------------------------------------------------------------------------

In this example question we ask: when tapping, is the brain region on the
contralateral side of the brain to the tapping hand larger than the
ipsilateral side?

First the ROI data in the dataframe is encoded as ipsi- and contralateral
to the tapping. Then the data is subset to just examine the tapping
conditions and the model is applied.

.. GENERATED FROM PYTHON SOURCE LINES 383-403

.. code-block:: default


    # Encode the ROIs as ipsi- or contralateral to the hand that is tapping.
    df["Hemishphere"] = "Unknown"
    df.loc[(df["Condition"] == "Tapping/Right") &
           (df["ROI"] == "Right_Hemisphere"), "Hemishphere"] = "Ipsilateral"
    df.loc[(df["Condition"] == "Tapping/Right") &
           (df["ROI"] == "Left_Hemisphere"), "Hemishphere"] = "Contralateral"
    df.loc[(df["Condition"] == "Tapping/Left") &
           (df["ROI"] == "Left_Hemisphere"), "Hemishphere"] = "Ipsilateral"
    df.loc[(df["Condition"] == "Tapping/Left") &
           (df["ROI"] == "Right_Hemisphere"), "Hemishphere"] = "Contralateral"

    # Subset the data for example model
    input_data = df.query("Condition in ['Tapping/Right', 'Tapping/Left']")
    input_data = input_data.query("Chroma in ['hbo']")

    roi_model = smf.mixedlm("Value ~ Hemishphere", input_data,
                            groups=input_data["ID"]).fit()
    roi_model.summary()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <table class="simpletable">
    <tr>
           <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>Value</td> 
    </tr>
    <tr>
      <td>No. Observations:</td>   <td>20</td>          <td>Method:</td>         <td>REML</td>  
    </tr>
    <tr>
         <td>No. Groups:</td>       <td>5</td>          <td>Scale:</td>         <td>7.1669</td> 
    </tr>
    <tr>
      <td>Min. group size:</td>     <td>4</td>      <td>Log-Likelihood:</td>   <td>-49.3029</td>
    </tr>
    <tr>
      <td>Max. group size:</td>     <td>4</td>        <td>Converged:</td>         <td>Yes</td>  
    </tr>
    <tr>
      <td>Mean group size:</td>    <td>4.0</td>            <td></td>               <td></td>    
    </tr>
    </table>
    <table class="simpletable">
    <tr>
                   <td></td>               <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>
    </tr>
    <tr>
      <th>Intercept</th>                   <td>8.357</td>   <td>1.636</td>   <td>5.108</td> <td>0.000</td>  <td>5.151</td> <td>11.564</td>
    </tr>
    <tr>
      <th>Hemishphere[T.Ipsilateral]</th> <td>-3.440</td>   <td>1.197</td>  <td>-2.873</td> <td>0.004</td> <td>-5.786</td> <td>-1.093</td>
    </tr>
    <tr>
      <th>Group Var</th>                   <td>9.800</td>   <td>3.472</td>     <td></td>      <td></td>       <td></td>       <td></td>   
    </tr>
    </table>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 404-407

And the model indicates that for the oxyhaemoglobin data that ipsilateral
responses are 3.4 μMol smaller than those on the contralateral side to the
hand that is tapping.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  9.114 seconds)

**Estimated memory usage:**  19 MB


.. _sphx_glr_download_auto_examples_general_plot_16_waveform_group.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_16_waveform_group.py <plot_16_waveform_group.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_16_waveform_group.ipynb <plot_16_waveform_group.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
